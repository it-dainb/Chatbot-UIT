{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.database.database import Database\n",
    "from core.config.config import get_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/itdainb/miniconda3/envs/cb_uit/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "Settings.llm = OpenAI(\n",
    "    model=\"gpt-3.5-turbo\", \n",
    "    temperature=0.0,\n",
    "    api_key=os.getenv(\"api_key\"),\n",
    "    api_version=os.getenv(\"api_version\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-15 00:38:29.023910: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-15 00:38:29.054432: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-15 00:38:29.054467: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-15 00:38:29.055729: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-15 00:38:29.061277: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-15 00:38:29.663608: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface_optimum import OptimumEmbedding\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "embed_path = str(Path(\n",
    "    get_config(\"Path\", \"model\"),\n",
    "    get_config(\"Model\", \"embed\")\n",
    "))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(embed_path)\n",
    "tokenizer.model_input_names = [\"input_ids\", \"attention_mask\"]\n",
    "\n",
    "Settings.embed_model = OptimumEmbedding(\n",
    "    folder_name=embed_path,\n",
    "    tokenizer=tokenizer,\n",
    "    pooling='mean',\n",
    "    max_length=256,\n",
    "    device='cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m00:38:31 [ INFO      ] CHATBOT_UIT: \u001b[0m \u001b[34mReading file\u001b[0m\n",
      "\u001b[32m00:38:31 [ INFO      ] CHATBOT_UIT: \u001b[0m \u001b[34mReading share knowledge\u001b[0m\n",
      "\u001b[32m00:38:31 [ INFO      ] CHATBOT_UIT: \u001b[0m \u001b[34mReading answer database\u001b[0m\n",
      "/home/itdainb/miniconda3/envs/cb_uit/lib/python3.10/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "\u001b[32m00:38:31 [ INFO      ] CHATBOT_UIT: \u001b[0m \u001b[34mReading product\u001b[0m\n",
      "/home/itdainb/miniconda3/envs/cb_uit/lib/python3.10/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "database = Database(\n",
    "    path_indomain=get_config(\"Path\", \"indomain\"),\n",
    "    path_outdomain=get_config(\"Path\", \"outdomain\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.utils.preprocessing import clean_text\n",
    "\n",
    "data = {}\n",
    "\n",
    "for idx, row in database.answer.iterrows():\n",
    "    answer = row[\"Answer\"]\n",
    "    pattern = row[\"Pattern Template\"]\n",
    "    intent = pattern.split(\"|\")[0]\n",
    "\n",
    "    if intent not in data:\n",
    "        data[intent] = {}\n",
    "\n",
    "    data[intent][pattern] = {\n",
    "        'answer': answer,\n",
    "        'question': []\n",
    "    }\n",
    "\n",
    "for idx, row in database.question.iterrows():\n",
    "    question = row[\"Question\"]\n",
    "    pattern = row[\"Pattern Template\"]\n",
    "    intent = row['Intent']\n",
    "\n",
    "    if intent not in data or pattern not in data[intent]: \n",
    "        continue\n",
    "\n",
    "    question = clean_text(question, database.synonyms_dictionary, tokenizer=False)\n",
    "    \n",
    "    data[intent][pattern]['question'].append(question)\n",
    "\n",
    "for intent in data.keys():\n",
    "    for pattern in data[intent].keys():\n",
    "        data[intent][pattern]['question'] = list(set(data[intent][pattern]['question']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data/clean_data.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib, pickle\n",
    "from os import path\n",
    "\n",
    "joblib.dump(data, path.join(get_config(\"Path\", \"data\"), \"clean_data.pkl\"), protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.schema import TextNode, IndexNode\n",
    "\n",
    "nodes_intent = {}\n",
    "nodes_dict = {}\n",
    "for intent, pattern_data in data.items():\n",
    "    \n",
    "    nodes = []\n",
    "    for pattern, value in pattern_data.items():\n",
    "        \n",
    "        node = TextNode(\n",
    "            text=value['answer'],\n",
    "            metadata = {\n",
    "                \"pattern\": pattern,\n",
    "            },\n",
    "            excluded_llm_metadata_keys=[\"pattern\"],\n",
    "        )\n",
    "        \n",
    "        nodes.append(node)\n",
    "        \n",
    "        nodes.extend([\n",
    "            IndexNode(\n",
    "                text=pattern,\n",
    "                index_id=node.node_id,\n",
    "            ),\n",
    "            IndexNode(\n",
    "                text=intent,\n",
    "                index_id=node.node_id,\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        for question in value['question']:\n",
    "            nodes.append(\n",
    "                IndexNode(\n",
    "                    text=question,\n",
    "                    index_id=node.node_id,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    nodes_intent[intent] = nodes\n",
    "    nodes_dict[intent] = {node.node_id: node for node in nodes}\n",
    "    \n",
    "len(nodes_intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext, VectorStoreIndex\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "from llama_index.core.schema import BaseNode\n",
    "from llama_index.core.storage.docstore.types import RefDocInfo\n",
    "\n",
    "class CustomDocumentStore(SimpleDocumentStore):\n",
    "    def _get_kv_pairs_for_insert(\n",
    "        self, node: BaseNode, ref_doc_info: Optional[RefDocInfo], store_text: bool\n",
    "    ) -> Tuple[\n",
    "        Optional[Tuple[str, dict]],\n",
    "        Optional[Tuple[str, dict]],\n",
    "        Optional[Tuple[str, dict]],\n",
    "    ]:\n",
    "        if isinstance(node, IndexNode):\n",
    "            return None, None, None\n",
    "        \n",
    "        return super()._get_kv_pairs_for_insert(node, ref_doc_info, store_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m00:38:49 [ INFO      ] chromadb.telemetry.product.posthog: \u001b[0m \u001b[34mAnonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\u001b[0m\n",
      "Generating embeddings: 100%|██████████| 256/256 [00:08<00:00, 29.85it/s]\n",
      "Generating embeddings: 100%|██████████| 256/256 [00:08<00:00, 28.72it/s]\n",
      "Generating embeddings: 100%|██████████| 74/74 [00:04<00:00, 18.37it/s]\n",
      "Generating embeddings: 100%|██████████| 256/256 [00:02<00:00, 123.61it/s]\n",
      "Generating embeddings: 100%|██████████| 256/256 [00:01<00:00, 144.30it/s]\n",
      "Generating embeddings: 100%|██████████| 84/84 [00:00<00:00, 118.06it/s]\n",
      "Generating embeddings: 100%|██████████| 256/256 [00:02<00:00, 126.51it/s]\n",
      "Generating embeddings: 100%|██████████| 256/256 [00:01<00:00, 128.70it/s]\n",
      "Generating embeddings: 100%|██████████| 256/256 [00:02<00:00, 92.41it/s] \n",
      "Generating embeddings: 100%|██████████| 256/256 [00:02<00:00, 97.97it/s] \n",
      "Generating embeddings: 100%|██████████| 82/82 [00:00<00:00, 116.18it/s]\n",
      "Generating embeddings: 100%|██████████| 235/235 [00:03<00:00, 63.71it/s]\n",
      "Generating embeddings: 100%|██████████| 38/38 [00:00<00:00, 40.86it/s]\n",
      "Generating embeddings: 100%|██████████| 256/256 [00:04<00:00, 54.84it/s]\n",
      "Generating embeddings: 100%|██████████| 34/34 [00:00<00:00, 57.29it/s]\n",
      "Generating embeddings: 100%|██████████| 199/199 [00:02<00:00, 79.41it/s]\n",
      "Generating embeddings: 100%|██████████| 185/185 [00:06<00:00, 28.43it/s]\n",
      "Generating embeddings: 100%|██████████| 256/256 [00:02<00:00, 93.86it/s] \n",
      "Generating embeddings: 100%|██████████| 214/214 [00:07<00:00, 27.21it/s] \n",
      "Generating embeddings: 100%|██████████| 3/3 [00:00<00:00, 92.28it/s]\n",
      "Generating embeddings: 100%|██████████| 101/101 [00:01<00:00, 57.54it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Index/nodes_dict.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import path\n",
    "from unidecode import unidecode\n",
    "import joblib, pickle\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(get_config(\"Path\", \"Index\"))\n",
    "\n",
    "for intent, nodes in nodes_intent.items():\n",
    "    intent = unidecode(intent.lower())\n",
    "    \n",
    "    chroma_collection = chroma_client.get_or_create_collection(intent)\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    vector_store.stores_text = False\n",
    "    \n",
    "    storage_context = StorageContext.from_defaults(\n",
    "        vector_store=vector_store,\n",
    "        docstore=CustomDocumentStore(),\n",
    "    )\n",
    "\n",
    "    index = VectorStoreIndex(nodes=nodes, storage_context=storage_context, show_progress=True, insert_batch_size=256)\n",
    "    \n",
    "    storage_context.persist(path.join(get_config(\"Path\", \"Index\"), intent))\n",
    "\n",
    "joblib.dump(nodes_dict, path.join(get_config(\"Path\", \"Index\"), \"nodes_dict.pkl\"), protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retriever(intent, **kargs):\n",
    "    intent = unidecode(intent.lower())\n",
    "    path_save = path.join(get_config(\"Path\", \"Index\"), intent)\n",
    "    \n",
    "    storage_context = StorageContext.from_defaults(\n",
    "        vector_store=ChromaVectorStore(\n",
    "            chroma_collection=chroma_client.get_or_create_collection(intent)\n",
    "        ),\n",
    "        persist_dir=path_save,\n",
    "    )\n",
    "    \n",
    "    index = VectorStoreIndex(\n",
    "        nodes = [],\n",
    "        storage_context = storage_context\n",
    "    )\n",
    "    \n",
    "    return index.as_retriever(**kargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mRetrieving with query id None: Điểm chuẩn ngành Khoa học máy tính 2022\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: 6896ce97-cf8f-4dad-a0e6-22a50604ecb0\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id 6896ce97-cf8f-4dad-a0e6-22a50604ecb0: Điểm chuẩn ngành Khoa học máy tính 2022\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: b96a06d4-1cce-4aaf-8dee-993119efafd2\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id b96a06d4-1cce-4aaf-8dee-993119efafd2: Điểm chuẩn ngành Khoa học máy tính 2022\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: fa34cd77-6b6b-44c5-809f-4255c40e5c75\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id fa34cd77-6b6b-44c5-809f-4255c40e5c75: Điểm chuẩn ngành Khoa học máy tính 2022\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: bfeecf33-4ad5-4e56-bf5a-3422e55d5a05\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id bfeecf33-4ad5-4e56-bf5a-3422e55d5a05: Điểm chuẩn ngành Khoa học máy tính 2022\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='6896ce97-cf8f-4dad-a0e6-22a50604ecb0', embedding=None, metadata={'pattern': 'hỏi_đáp_điểm_chuẩn|dgnl|khmt|năm_2022'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=['pattern'], relationships={}, text='Điểm chuẩn ĐGNL năm 2022 ngành Khoa học máy tính là 888', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.7072114388823445),\n",
       " NodeWithScore(node=TextNode(id_='b96a06d4-1cce-4aaf-8dee-993119efafd2', embedding=None, metadata={'pattern': 'hỏi_đáp_điểm_chuẩn|thpt|khmt|năm_2021'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=['pattern'], relationships={}, text='điểm chuẩn ngành Khoa học máy tính năm 2021 là 27.3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.7024271121756573),\n",
       " NodeWithScore(node=TextNode(id_='fa34cd77-6b6b-44c5-809f-4255c40e5c75', embedding=None, metadata={'pattern': 'hỏi_đáp_điểm_chuẩn|thpt|khmt|năm_2022'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=['pattern'], relationships={}, text='Điểm chuẩn ngành Khoa học máy tính năm 2022 là 27.1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6774684763139209),\n",
       " NodeWithScore(node=TextNode(id_='bfeecf33-4ad5-4e56-bf5a-3422e55d5a05', embedding=None, metadata={'pattern': 'hỏi_đáp_điểm_chuẩn|thpt|khmt|ttnt|năm_2022'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=['pattern'], relationships={}, text='Điểm chuẩn ngành Khoa học máy tính năm 2022 hướng Trí tuệ nhân tạo là 28', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6492161025307642)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.retrievers import RecursiveRetriever\n",
    "\n",
    "intent = \"hỏi_đáp_điểm_chuẩn\"\n",
    "\n",
    "vector_retriever = get_retriever(intent, similarity_top_k=10)\n",
    "\n",
    "retriever = RecursiveRetriever(\n",
    "    \"vector\",\n",
    "    retriever_dict={\"vector\": vector_retriever},\n",
    "    node_dict=nodes_dict[intent],\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "retriever.retrieve(\"Điểm chuẩn ngành Khoa học máy tính 2022\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cb_uit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
